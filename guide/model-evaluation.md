# 模型评估

目前平台支持基线评估。基线评估涉及使用预置数据集（如 CMMLU、MMLU、C-Eval）和自定义数据集来评估模型的通用能力。预置数据集提供了一个广泛认可的标准来测试模型在不同学科上的知识理解和推理能力，而自定义数据集则允许用户针对特定任务或领域进行评估。基线评估的结果可以用来与对比模型的性能进行比较，以确定模型优化的效果。

## 创建评估任务

点击产品「模型服务平台 UModelVerse」—— 功能菜单「模型评估」—— 创建评估任务

## 评估设置

1. 评估方式：默认当前为「基线评估」。
2. 选择模型来源：「预置数据集」或「自定义数据集」。
- 若选择「预置数据集」，则在 CMMLU、MMLU、C-Eval 中选择一个合适的数据集，数据集特性如下：

| 项目      | CMMLU                                                                 | MMLU                                                                 | C-Eval                                                                 |
|-----------|----------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|
| 定义      | 一个综合性的中文评估基准，专门用于评估语言模型在中文语境下的知识和推理能力。 | 一个大规模多任务语言理解基准，旨在通过零样本和少样本设置评估模型获得的知识。 | 一个全面的中文基础模型评估套件，包含 13948 个多项选择题，覆盖 52 个不同学科和四个难度级别。 |
| 学科覆盖  | 67 个主题，涵盖自然科学、社会科学、工程、人文以及常识等。 | 57 个科目，包括 STEM、人文科学、社会科学等领域。 | 52 个学科小类，涵盖 STEM、社会科学、人文科学和其他学科。 |
| 题目类型 | 单项选择题 | 多项选择题 | 多项选择题 |
| 适用语言  | 中文 | 英文 | 中文 |
| 特点      | 包括中国特色内容，如“中国饮食文化”、“名族学”、“中国驾驶规则”等。 | 测试内容既涵盖世界知识，又涉及问题解决能力。 | 针对中文模型的综合测试机。 |

   - 若选择「自定义数据集」，则在下拉框中选择一个在「数据集管理」中创建完成的数据集。如果还没有创建任何自定义数据集，系统将引导跳转至「数据集管理」页面进行创建。
3. 选择评估模型：必须选定一个基线模型，并可选择对比模型进行效果对比。基线模型和对比模型均支持选择「预置模型」或「我的模型」，或两种模型混合选择。

![model-evaluation1](https://www-s.ucloud.cn/2025/01/e3334ff6f462e303be21793a40892808_1736846738710.png)

4. 存储设置：通过授权令牌，选择 bucket 关联的令牌列表，目前仅支持选择华北二区域下的存储空间。

![model-evaluation2](https://www-s.ucloud.cn/2025/01/5c6aa18a3dd688bf1cdad6f2cdfbaf02_1736846738712.png)

## 管理任务和查看报告

在创建任务后，您可以查看任务管理列表。可对任务执行的操作有：查看报告、终止、删除。

### 查看报告

![model-evaluation3](https://www-s.ucloud.cn/2025/01/029a3c76110c913681932c548eff6048_1736846738708.png)

- 对于基线评估数据集，支持查看含有指标评分和可视化雷达图的报告。
- 对于自定义数据集，支持查看指标评分、雷达图，和单句得分详情。在查看评估详情时，平台支持对模型进行过滤筛选及单句评分排序。

![model-evaluation4](https://www-s.ucloud.cn/2025/01/82c9963d5219ce1a355db25f7150274b_1736846738724.png)

### 关于评估报告指标

- 预置数据集评估指标：

| 评分项         | 描述                                                      |
|---------------|--------------------------------------------------------------|
| **STEM**      | 模型在科学、技术、工程、数学学科上的处理能力。               |
| **Social Sciences** | 模型在社会科学科上的处理能力。                             |
| **Humanities** | 模型在人文学科上的处理能力。                                 |
| **Other**     | 模型在其他学科上的处理能力。                                 |
| **Average**   | 各项指标的权重综合得分。                                     |

- 自定义数据集评估指标：

| 评分项    | 描述                                                                 |
|-----------|----------------------------------------------------------------------|
| **ROUGE-1** | （忽略停用词后）将模型生成的结果和标准结果按 unigram 拆分后，计算出的召回率。 |
| **ROUGE-2** | （忽略停用词后）将模型生成的结果和标准结果按 bigram 拆分后，计算出的召回率。 |
| **ROUGE-L** | （忽略停用词后）衡量了模型生成的结果和标准结果的最长公共子序列，并计算出召回率。 |
| **BLEU**   | （忽略停用词后）用于评估模型生成的句子和实际句子的差异的指标，值为 unigram，bigram，trigram，4-grams 的加权平均。 |
| **注释**   | Ⅰ) unigram：指将句子或文本中的每个单词都单独作为一个基本单元，不考虑单词之间的顺序。 |
|           | Ⅱ) bigram：指将句子或文本中的每个相邻的单词对都作为一个基本单元，用于描述两个单词之间的顺序关系。 |
|           | Ⅲ) trigram：指将句子或文本中的每个相邻的三个单词作为一个基本单元，用于描述三个单词之间的顺序关系。 |
|           | Ⅳ) 4-grams：指将句子或文本中的每个相邻的四个单词作为一个基本单元，用于描述四个单词之间的顺序关系。 |
|           | Ⅴ) 最长公共子序列：指两个或多个字符串最长的子序列，这些子序列在每个字符串中都存在，且它们的顺序相同。 |

### 任务终止操作

- 终止：用户可以在评估任务排队中或进行中随时终止当前任务。

![model-evaluation5](https://www-s.ucloud.cn/2025/01/4fc6213ca15e67ac7f67f5eaf95dfa9a_1736846738729.png)

- 删除：对训练失败、已完成、已终止的任务可进行删除。

![model-evaluation6](https://www-s.ucloud.cn/2025/01/0ffccbdecb94181e9d453f4cb5bdff2d_1736846738740.png)
